{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, BertModel, AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, file_path):\n",
    "        self.dataset = pd.read_csv(file_path, sep='\\t')\n",
    "        \n",
    "        # drop nan row\n",
    "        self.dataset = self.dataset.dropna(axis = 0)\n",
    "        # drop duplicate row\n",
    "        self.dataset['document'] = self.dataset['document'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "        self.dataset.drop_duplicates(subset=['document'], inplace=True)\n",
    "        \n",
    "        # tokenizer\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "  \n",
    "        # add special tokens\n",
    "        self.tokenizer.add_tokens([\"OO\", \"OOO\", \"OOOO\"], special_tokens=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx, 1:3].values # no ids!\n",
    "    \n",
    "        '''\n",
    "        [\"document\", \"label\"]\n",
    "        '''\n",
    "        document = row[0]\n",
    "        label = row[1]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            document, \n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "            )\n",
    "\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NSMCDataset(\"train.txt\")\n",
    "test_dataset = NSMCDataset(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "        self.fc = nn.Linear(768, 2)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert_model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        return self.fc(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(42003, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:7\")\n",
    "model = BertClassifier()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.bert_model.resize_token_embeddings(len(train_dataset.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db9a3da53544d6d9ee23ec56e3f5c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0 500 Batch Loss:187.48752443119884 Accuracy:0.8335000276565552\n",
      "epoch #0 1000 Batch Loss:337.8686069510877 Accuracy:0.8538750410079956\n",
      "epoch #0 1500 Batch Loss:486.34194169938564 Accuracy:0.8603333234786987\n",
      "epoch #0 2000 Batch Loss:623.1496058255434 Accuracy:0.8673437833786011\n",
      "epoch #0 2500 Batch Loss:753.1026105880737 Accuracy:0.8728249669075012\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_index = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_batch = y_batch.to(device)\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attention_masks_batch = attention_masks_batch.to(device)\n",
    "        \n",
    "        y_pred = model(input_ids=input_ids_batch, attention_mask=attention_masks_batch)\n",
    "        loss = F.cross_entropy(y_pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y_batch).sum()\n",
    "        total += len(y_batch)\n",
    "\n",
    "        batch_index += 1\n",
    "        if batch_index % 500 == 0:\n",
    "            print(f\"epoch #{epoch} {batch_index} Batch Loss:{total_loss} Accuracy:{correct.float() / total}\")\n",
    "  \n",
    "    losses.append(total_loss)\n",
    "    accuracies.append(correct.float() / total)\n",
    "    print(\"Train Loss:\", total_loss, \"Accuracy:\", correct.float() / total)\n",
    "    torch.save(model.state_dict(), f\"model_{i}_v2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(loader):\n",
    "    y_batch = y_batch.to(device)\n",
    "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))\n",
    "    predicted = torch.max(y_pred, 1)[1]\n",
    "    test_correct += (predicted == y_batch).sum()\n",
    "    test_total += len(y_batch)\n",
    "\n",
    "print(\"Accuracy:\", test_correct.float() / test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
