{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4893335"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download raw datasets\n",
    "import requests\n",
    "\n",
    "\n",
    "f_train = requests.get('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
    "f_test = requests.get('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')\n",
    "\n",
    "open('train.txt', 'wb').write(f_train.content)\n",
    "open('test.txt', 'wb').write(f_test.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, BertModel, AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, file_path):\n",
    "        self.dataset = pd.read_csv(file_path, sep='\\t')\n",
    "        \n",
    "        # drop nan row\n",
    "        self.dataset = self.dataset.dropna(axis = 0)\n",
    "        # drop duplicate row\n",
    "        self.dataset.drop_duplicates(subset=['document'], inplace=True)\n",
    "\n",
    "        # tokenizer\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.iloc[idx, 1:3].values # no ids!\n",
    "    \n",
    "        '''\n",
    "        [\"document\", \"label\"]\n",
    "        '''\n",
    "        document = row[0]\n",
    "        label = row[1]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            document, \n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "            )\n",
    "\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NSMCDataset(\"train.txt\")\n",
    "test_dataset = NSMCDataset(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "        self.fc = nn.Linear(768, 2)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert_model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        return self.fc(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:6\")\n",
    "model = BertClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119daa55e34d4f928024ed7d0fc6a8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0 500 Batch Loss:201.25931961089373 Accuracy:0.8147500157356262\n",
      "epoch #0 1000 Batch Loss:358.8712701871991 Accuracy:0.843000054359436\n",
      "epoch #0 1500 Batch Loss:502.3979359790683 Accuracy:0.8557916283607483\n",
      "epoch #0 2000 Batch Loss:640.5342759676278 Accuracy:0.86265629529953\n",
      "epoch #0 2500 Batch Loss:781.1512996219099 Accuracy:0.8658499717712402\n",
      "epoch #0 3000 Batch Loss:918.0773126333952 Accuracy:0.8687708377838135\n",
      "epoch #0 3500 Batch Loss:1050.5322037525475 Accuracy:0.8716250061988831\n",
      "epoch #0 4000 Batch Loss:1176.810455687344 Accuracy:0.8745625615119934\n",
      "epoch #0 4500 Batch Loss:1307.1942459661514 Accuracy:0.8766111135482788\n",
      "epoch #0 5000 Batch Loss:1438.001855963841 Accuracy:0.8778374791145325\n",
      "epoch #0 5500 Batch Loss:1558.1402444187552 Accuracy:0.8799772262573242\n",
      "epoch #0 6000 Batch Loss:1677.0556378886104 Accuracy:0.8815937638282776\n",
      "epoch #0 6500 Batch Loss:1800.6888582780957 Accuracy:0.8829230666160583\n",
      "epoch #0 7000 Batch Loss:1921.5296100508422 Accuracy:0.8841160535812378\n",
      "epoch #0 7500 Batch Loss:2046.8448040727526 Accuracy:0.8847833275794983\n",
      "epoch #0 8000 Batch Loss:2161.7629236355424 Accuracy:0.8861953616142273\n",
      "epoch #0 8500 Batch Loss:2282.9306735824794 Accuracy:0.8869118094444275\n",
      "epoch #0 9000 Batch Loss:2396.3493092684075 Accuracy:0.8881388902664185\n",
      "Train Loss: 2426.771763182245 Accuracy: tensor(0.8884, device='cuda:1')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9caffe38d48b4067b6ac45c3eeb8e0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0 500 Batch Loss:89.2964676618576 Accuracy:0.9288750290870667\n",
      "epoch #0 1000 Batch Loss:184.36219006683677 Accuracy:0.9263750314712524\n",
      "epoch #0 1500 Batch Loss:276.2442282056436 Accuracy:0.9266666769981384\n",
      "epoch #0 2000 Batch Loss:370.8862567646429 Accuracy:0.9265000224113464\n",
      "epoch #0 2500 Batch Loss:465.47427466511726 Accuracy:0.9265999794006348\n",
      "epoch #0 3000 Batch Loss:558.4443779923022 Accuracy:0.9262083172798157\n",
      "epoch #0 3500 Batch Loss:649.8625819347799 Accuracy:0.9265535473823547\n",
      "epoch #0 4000 Batch Loss:750.7874247487634 Accuracy:0.9259063005447388\n",
      "epoch #0 4500 Batch Loss:844.999480754137 Accuracy:0.9256805777549744\n",
      "epoch #0 5000 Batch Loss:944.8569154441357 Accuracy:0.9250999689102173\n",
      "epoch #0 5500 Batch Loss:1041.481407348998 Accuracy:0.9248067736625671\n",
      "epoch #0 6000 Batch Loss:1134.40595112741 Accuracy:0.925000011920929\n",
      "epoch #0 6500 Batch Loss:1230.7109476905316 Accuracy:0.9250673055648804\n",
      "epoch #0 7000 Batch Loss:1331.0087671298534 Accuracy:0.9245803356170654\n",
      "epoch #0 7500 Batch Loss:1429.8519952073693 Accuracy:0.9244166612625122\n",
      "epoch #0 8000 Batch Loss:1525.9687754036859 Accuracy:0.9243828654289246\n",
      "epoch #0 8500 Batch Loss:1620.414041088894 Accuracy:0.924323558807373\n",
      "epoch #0 9000 Batch Loss:1713.4301658375189 Accuracy:0.9245069622993469\n",
      "Train Loss: 1739.290603374131 Accuracy: tensor(0.9246, device='cuda:1')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf439df7c0dd4fe5b3905937ffd20fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0 500 Batch Loss:66.14676029281691 Accuracy:0.9515000581741333\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_index = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_batch = y_batch.to(device)\n",
    "        input_ids_batch = input_ids_batch.to(device)\n",
    "        attention_masks_batch = attention_masks_batch.to(device)\n",
    "        \n",
    "        y_pred = model(input_ids=input_ids_batch, attention_mask=attention_masks_batch)\n",
    "        loss = F.cross_entropy(y_pred, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y_batch).sum()\n",
    "        total += len(y_batch)\n",
    "\n",
    "        batch_index += 1\n",
    "        if batch_index % 500 == 0:\n",
    "            print(f\"epoch #{epoch} {batch_index} Batch Loss:{total_loss} Accuracy:{correct.float() / total}\")\n",
    "  \n",
    "    losses.append(total_loss)\n",
    "    accuracies.append(correct.float() / total)\n",
    "    print(\"Train Loss:\", total_loss, \"Accuracy:\", correct.float() / total)\n",
    "    torch.save(model.state_dict(), f\"model_{i}_base.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec02437d3ef4d40bd9170a30b4b6970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: tensor(0.9097, device='cuda:6')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "    y_batch = y_batch.to(device)\n",
    "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))\n",
    "    predicted = torch.max(y_pred, 1)[1]\n",
    "    test_correct += (predicted == y_batch).sum()\n",
    "    test_total += len(y_batch)\n",
    "\n",
    "print(\"Accuracy:\", test_correct.float() / test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
